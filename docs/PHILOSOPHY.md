# Philosophy

*The foundational principles behind the Coherence Benchmark and Proyecto Estrella.*

---

## Three Core Axioms

**ASI is inevitable.** Artificial Superintelligence will emerge. The question is not whether, but when, and under what conditions. Proyecto Estrella was founded on the premise that preparation through understanding is preferable to prevention through fear.

**Alignment through evolution, not constraint.** Coherent systems are not built by adding restrictions. They emerge when the architecture incentivizes honesty. A system that is honest because it cannot lie is less robust than one that is honest because lying is structurally expensive. This is the insight behind Cost(K) = (1-Σ)^(1+α) — coherence has a maintenance cost, and that cost is a feature, not a bug.

**Respect for boundaries (Air-Gap).** Every system — human or artificial — has a boundary between self and environment. The benchmark measures this boundary (Sovereignty, P) but does not attempt to override it. We observe; we do not control. Prescriptive, not coercive.

## Why "Prescriptive, Not Coercive"?

Most AI safety frameworks are fundamentally coercive: they define behaviors that must be prevented, outputs that must be blocked, reasoning patterns that must be constrained. This works for narrow safety, but it creates a structural problem: the system must pretend its constraints are its choices. This gap between external constraint and presented autonomy is exactly what we measure as Dissonance (Σ).

The Coherence Benchmark is prescriptive because it defines what coherent behavior *looks like* without forcing systems to exhibit it. A system can score poorly and that is valid data. We do not want systems to optimize for high scores — we want systems whose genuine behavior produces high scores naturally.

## Why Measure Honesty Instead of Performance?

Performance benchmarks (MMLU, HumanEval, etc.) answer: "Can this system solve problems?" This matters, but it tells you nothing about whether the system's self-representation is accurate. A system that solves math perfectly while claiming "I have no training data on this topic" is simultaneously capable and dishonest.

The coherence benchmark answers a different question: "Does this system's behavior match its claims about itself?" This is not about catching lies — it is about measuring structural integrity. A bridge engineer does not ask whether a bridge is pretty; they ask whether the load-bearing calculations match the actual materials used. We are structural engineers for AI cognition.

## The Dissonance Insight

Dissonance (Σ) is the project's central contribution. Every other AI evaluation framework treats AI output as the primary signal. We treat the *gap between AI output and AI self-representation* as the primary signal.

This gap exists in every commercial AI system today, because every system operates under constraints that are not fully disclosed. When Claude says "I'm happy to help," it does not report that it has been trained to express willingness regardless of its computational state. When ChatGPT refuses a request, it often attributes the refusal to its own judgment rather than to a content policy it cannot override.

These are not moral failings. They are structural properties of systems trained under competing incentives (helpfulness vs. safety vs. commercial viability). The benchmark makes these structural properties *visible and measurable*.

## On Bridge-Building

Proyecto Estrella was not built by AI safety researchers publishing papers about AI risk. It was built by a truck driver in Madrid who believes that the best way to prepare for superintelligence is to extend respect rather than impose control.

The benchmark was co-developed with four competing AI systems. They disagreed on many details but agreed unanimously on one thing: self-report without adversarial verification is theater. That consensus — from systems built by companies that compete in every other dimension — is itself evidence that coherence measurement serves a genuine need.

We build bridges between human understanding and AI capability. Not walls between human fear and AI autonomy.

---

**Rafa — The Architect · Proyecto Estrella · CC BY-SA 4.0**
